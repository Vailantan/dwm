from bs4 import BeautifulSoup
import json
import pandas as pd
from copy import deepcopy
from urllib.request import urlopen


scrape_url = 'https://en.wikipedia.org/wiki/Chicken '
page_connect = urlopen(scrape_url)
page_html = BeautifulSoup(page_connect, 'html.parser')

page_html

page_html.p.get_text()

data_content = page_html.p.get_text()
data_content

temp = page_html.find_all('p')

temp 

for p in temp:
  data_content += p.text

data_content

import spacy
from spacy.lang.en.stop_words import STOP_WORDS 
from string import punctuation
from collections import Counter
from heapq import nlargest

nlp = spacy.load('en_core_web_sm')

doc = nlp(data_content)

len(list(doc.sents))

keyword = []
stopwords = list(STOP_WORDS)
pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']
for token in doc:
  if(token.text in stopwords or token.text in punctuation):
    continue
  if(token.pos_ in pos_tag):
    keyword.append(token.text)

freq_word = Counter(keyword)
freq_word.most_common(5)

max_freq = Counter(keyword).most_common(1)[0][1]
for word in freq_word.keys():
  freq_word[word] = (freq_word[word] / max_freq)
freq_word.most_common(5)

sent_strength = {}
for sent in doc.sents:
  for word in sent:
    if word.text in freq_word.keys():
      if sent in sent_strength.keys():
        sent_strength[sent] += freq_word[word.text]
      else:
        sent_strength[sent] = freq_word[word.text]
print(sent_strength)

print(sent_strength.values())

summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)
print(summarized_sentences)

print(summarized_sentences[0])

final_sentences = [ w.text for w in summarized_sentences]
summary = ' '.join(final_sentences)

print(f"Length of original content: {len(data_content)}\nLength of summarized content: {len(summary)}")
